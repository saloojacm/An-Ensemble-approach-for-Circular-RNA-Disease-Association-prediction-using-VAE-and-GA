# -*- coding: utf-8 -*-
"""Copy of v23_GAAE_AugmentationCDAipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CyyxelKY4lS5AURNbKAN-DI_TE4otZi6

VAGA CDA
"""

from google.colab import drive
drive.mount('/content/drive')

import sklearn ,tensorflow
from tensorflow import keras
import tensorflow as tf
import keras
import keras.activations
import keras.layers
#from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense)
import numpy as np
from tensorflow.keras import layers
# -*- coding: utf-8 -*-
###THEANO_FLAGS=mode=FAST_RUN,device=gpu0,floatX=float32 python
import numpy as np
from matplotlib import pyplot
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.decomposition import PCA
from sklearn import metrics
from sklearn import model_selection

from sklearn.neighbors import KNeighborsClassifier
from keras import models
from sklearn.model_selection import train_test_split
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
import pandas as pd
from keras.layers import Input, Dense
#from keras.engine.training import Model
from keras.models import Sequential, model_from_config,Model
#from keras.layers.core import  Dropout, Activation, Flatten
from keras.optimizers import SGD, RMSprop, Adadelta, Adagrad, Adam
#from keras.layers import normalization
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.model_selection import KFold, cross_val_score
from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from matplotlib import pyplot
from numpy import where
from keras.layers import AveragePooling1D,AveragePooling2D
import random
!pip install sklearn_genetic
#from keras.layers.serialization import activation
from genetic_selection import GeneticSelectionCV
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential, model_from_config,Model
from sklearn import datasets,svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score,f1_score,auc,RocCurveDisplay
from sklearn.metrics import precision_score,recall_score,matthews_corrcoef
from sklearn.model_selection import train_test_split,StratifiedKFold
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
!pip install imblearn

def transfer_array_format(data):    #data=X  , X= all the circRNA features, disease features
    formated_matrix1 = []
    formated_matrix2 = []

    for val in data:

        formated_matrix1.append(val[0])   #contains circRNA features
        formated_matrix2.append(val[1])   #contains disease features


    return np.array(formated_matrix1), np.array(formated_matrix2)

# Oversample with SMOTE and random undersample for imbalanced dataset

def smotedata():
  disease_fea = np.loadtxt("/content/drive/MyDrive/AECDA/integrated Disease Similarity.txt",dtype=float,delimiter=",")
  circRNA_fea  = np.loadtxt("/content/drive/MyDrive/AECDA/integrated CircRNA  Similarity.txt",dtype=float,delimiter=",")
  interaction = np.loadtxt("/content/drive/MyDrive/AECDA/Association matrix.txt",dtype=int,delimiter=",")

  train = []
  test = []
  labela = []
  labeln = []
  indexAssoc = []
  indexNonassoc = []
  data_matrix=[]
  alllabel=[]
  i=0
  j=0
  for i in range(0, interaction.shape[0]):   # shape[0] returns m if interaction is m*n, ie, returns no. of rows of matrix
        for j in range(0, interaction.shape[1]):

            if interaction[i, j] == 1:                      #for associated
                labela.append(interaction[i,j])             #labela= labels for association(1)
                ind=[]
                ind.append(i+1)
                ind.append(j+1)
                indexAssoc.append(ind)
                circRNA_fea_list = list(circRNA_fea[i])
                disease_fea_list = list(disease_fea[j])
                fea_vect = (circRNA_fea_list,disease_fea_list)   #concatnated feature vector for an association
                train.append(fea_vect)                  #train contains feature vectors of all associated samples
                data_matrix.append(fea_vect)
                ind.append(1)
                alllabel.append(ind)
            elif interaction[i,j] == 0:                     #for no association
                labeln.append(interaction[i,j])             #labeln= labels for no association(0)
                ind=[]
                ind.append(i+1)
                ind.append(j+1)
                indexNonassoc.append(ind)
                circRNA_nonfea = list(circRNA_fea[i])
                disease_nonfea = list(disease_fea[j])
                test_fea= (circRNA_nonfea,disease_nonfea) #concatenated feature vector for not having association
                test.append(test_fea)                    #test contains feature vectors of all non associated samples
                data_matrix.append(test_fea)
                ind.append(0)
                alllabel.append(ind)
  df3=pd.DataFrame(data_matrix)
  df3.to_csv("datamatrix.csv")
  df4=pd.DataFrame(alllabel)
  df4.to_csv("labelmatrix.csv")
  print("Data outputs")
  print("associated data len:",len(train))
  print("non associated len:",len(test))
  print("label len:",len(labela))
  return train,test,labela,labeln,indexAssoc,indexNonassoc,data_matrix,alllabel

def transfer_label_from_prob(proba):
    label = [1 if val>=0.5 else 0 for val in proba]
    return label

XX,TT,laba,labn,assoc_index,nonassoc_index,data_matrix,alllabel = smotedata() #XX-associated(650),TT-nonassociated(50830)
SS=random.sample(TT,k=1300)
ll=random.sample(labn,k=1300)
DD=XX+SS                    #DD-650+1300=1950
x1,x2=transfer_array_format(DD)    #x1 circRNAfea, x2 diseasefea
x1= np.concatenate((x1, x2), axis = 1) #axis=1 , rowwoise concatenation x1-(1950,650)
labels=laba+ll        #labels-1950

#split data
X_train, X_test, y_train, y_test = train_test_split(x1,np.array(labels), test_size=0.2, random_state=42)

print("SMOTE RES X")
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_train, y_train)

#-----------Variational Auto Encoder------------

# this sampling layer is the bottleneck layer of variational autoencoder,
# it uses the output from two dense layers z_mean and z_log_var as input,
# convert them into normal distribution and pass them to the decoder layer
class Sampling(layers.Layer):

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape =(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

input_size = 673
latent_dim = 128

input_data = Input(shape=(input_size,))
encoded = Dense(512, activation='relu')(input_data)
encoded = Dense(256, activation='relu')(encoded)
z_mean = Dense(latent_dim, name ="z_mean")(encoded)
z_log_var = Dense(latent_dim, name ="z_log_var")(encoded)
z = Sampling()([z_mean, z_log_var])
encoder = Dense(latent_dim,activation='relu' )(z)

code = Dense(latent_dim, activation='relu')(encoder)
decoded = Dense(256, activation='relu')(code)
decoded = Dense(512, activation='relu')(decoded)
output_data = Dense(673, activation='sigmoid')(decoded)
autoencoder = Model(input_data, output_data)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')
autoencoder.fit(X_res,X_res, epochs=7)

##---------------Genetic Algorithm--------------

def GA(VAEoutput_data,y_train):
  estimator = DecisionTreeClassifier(max_depth=2)
  Rmodel = GeneticSelectionCV(
    estimator, verbose=2,
    scoring="accuracy", max_features=256,
    n_population=70, crossover_proba=0.2,
    mutation_proba=0.4, n_generations=75,
    crossover_independent_proba=0.3,
    mutation_independent_proba=0.04,
    tournament_size=3, n_gen_no_change=25,
    caching=True,
    n_jobs=-1)
  Rmodel = Rmodel.fit(VAEoutput_data,y_train)

  #print('features',Rmodel.support_)
  encoder = LabelEncoder()

  GAselectedfea=encoder.fit_transform(Rmodel.support_)
  return GAselectedfea

def GAdata(VAEoutput_data,GAselectedfea):
  count_arr = np.bincount(GAselectedfea)    #count no of 1s
  count_arr[1]
  new=[]
  Gfea=[]
  c=0
  for i in range(VAEoutput_data.shape[0]):
    new=[]
    feature=list(VAEoutput_data[i])
    for c in range(VAEoutput_data.shape[1]):
       if (GAselectedfea[c]==1):
          new.append(feature[c])

    Gfea.append(np.array(new))
  return Gfea

import matplotlib.pyplot as plt

from sklearn import svm
from sklearn.metrics import auc
from sklearn.metrics import RocCurveDisplay
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_recall_curve

from sklearn.metrics import precision_score,recall_score,f1_score,matthews_corrcoef
from imblearn.metrics import sensitivity_score,specificity_score
#*****************Training******************
VAE=autoencoder.predict(X_res)
GAselectedfea=GA(VAE,y_res)
Gfeature=GAdata(VAE,GAselectedfea)
Gfeature=np.array(Gfeature).astype(float)

#********testing****************

Testmodel=autoencoder.predict(X_test)
Gftest=GAdata(Testmodel,GAselectedfea)
Gftest=np.array(Gftest).astype(float)
outputModel1=RandomForestClassifier(n_estimators=200,random_state=42)
outputModel1.fit(Gfeature, y_train)
yprob1=outputModel1.predict(Gftest)
print("*************************************")
test_acc=accuracy_score(y_test,yprob1)
print('Test Accuracy:',test_acc)
#precision, recall, pr_threshods = precision_recall_curve(y_test, yprob1)
prec=precision_score(y_test,yprob1)
print('precision:',prec)
rec=recall_score(y_test,yprob1)
print('Recall:',rec)
f1value=f1_score(y_test,yprob1)
print('F1 value:',f1value)
#sensit_score=sensitivity_score(y_test,yprob1)
#print('Sensitivity Score :',sensit_score)
specif_score=specificity_score(y_test,yprob1)
print('Specificity Score :',specif_score)
mccscore=matthews_corrcoef(y_test,yprob1)
print('MCC Score :',mccscore)
yprob1=outputModel1.predict_proba(Gftest)[:,1]
auc_score = roc_auc_score(y_test,yprob1)
#mean_auc = metrics.auc(mean_fpr, mean_tpr)
print('AUC score:',auc_score)
#print('Mean AUC score:',mean_auc)
auprc = sklearn.metrics.average_precision_score(y_test,yprob1)
print('AUPR score:',auprc)

auprc = sklearn.metrics.average_precision_score(y_test,yprob1)
print(' AUPR score:',auprc)

outputModel1=RandomForestClassifier(n_estimators=200,random_state=50)
outputModel1.fit(Gfeature, y_train)
yprob1=outputModel1.predict_proba(Gftest)[:,1]
auc = roc_auc_score(y_test, yprob1)
print('AUC: %.3f' % auc)

outputModel2=KNeighborsClassifier(3)
outputModel2.fit(Gfeature, y_train)
yprob2=outputModel2.predict_proba(Gftest)[:,1]
auc = roc_auc_score(y_test, yprob2)
print('AUC: %.3f' % auc)

outputModel3=DecisionTreeClassifier(max_depth=1000)
outputModel3.fit(Gfeature, y_train)
yprob3=outputModel3.predict_proba(Gftest)[:,1]
auc = roc_auc_score(y_test, yprob3)
print('AUC: %.3f' % auc)

outputModel4=AdaBoostClassifier(n_estimators=20)
outputModel4.fit(Gfeature, y_train)
yprob4=outputModel4.predict_proba(Gftest)[:,1]
auc = roc_auc_score(y_test, yprob4)
print('AUC: %.3f' % auc)

outputModel5=GradientBoostingClassifier(n_estimators=200)
outputModel5.fit(Gfeature, y_train)
yprob5=outputModel5.predict_proba(Gftest)[:,1]
auc = roc_auc_score(y_test, yprob5)
print('AUC: %.3f' % auc)

#fig, ax = plt.subplots(figsize=(5, 5))
plt.rcParams.update({'font.size': 10})
pyplot.title('The ROC curve with different classifiers')
#pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
fpr1,tpr1,thr1 = roc_curve(y_test,yprob1)
auc = metrics.auc(fpr1, tpr1)
pyplot.plot(fpr1, tpr1,linestyle='solid', alpha=1,label='VAGA-CDA(Proposed model)(AUC=%0.4f)'%auc)
fpr2,tpr2,thr2 = roc_curve(y_test,yprob2)
auc = metrics.auc(fpr2, tpr2)
pyplot.plot(fpr2, tpr2,linestyle='solid',  alpha=1,label='KNN(AUC=%0.4f)'%auc)
fpr3,tpr3,thr3 = roc_curve(y_test,yprob3)
auc = metrics.auc(fpr3, tpr3)
#pyplot.plot(fpr3, tpr3,linestyle='solid', alpha=1, label='Decision tree(AUC=%0.4f)'%auc)
fpr4,tpr4,thr4 = roc_curve(y_test,yprob4)
auc = metrics.auc(fpr4, tpr4)
pyplot.plot(fpr4, tpr4,linestyle='solid', alpha=1, label='AdaBoost(AUC=%0.4f)'%auc)
fpr5,tpr5,thr5 = roc_curve(y_test,yprob5)
auc = metrics.auc(fpr5, tpr5)
pyplot.plot(fpr5, tpr5,linestyle='solid', alpha=1, label='Gradient Boosting(AUC=%0.4f)'%auc)

# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

stop run

case1,case2=transfer_array_format(np.array(TT))
case1= np.concatenate((case1, case2 ), axis = 1)
Testcase=test_GA(autoencoder,case1,GAmodel,GAselectedfea)
caseoutput=outputModel.predict_proba(Testcase)[:,1]

case3,case4=transfer_array_format(np.array(data_matrix))
case3= np.concatenate((case3, case4 ), axis = 1)
Testmodelcase=autoencoder.predict(case3)
testcase=GAdata(Testmodelcase,GAselectedfea)
testcase=np.array(testcase).astype(float)
caseoutput=outputModel1.predict_proba(testcase)[:,1]

# convert array into dataframe
DF = pd.DataFrame(caseoutput)
DF2=pd.DataFrame(nonassoc_index)
# save the dataframe as a csv file
DF.to_csv("data.csv")
DF2.to_csv("new_assoc.csv")
caseoutput

cv = StratifiedKFold(n_splits=5,shuffle=True)
classifier = RandomForestClassifier(n_estimators=100,random_state=48)

mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)
i=1
fig, ax = plt.subplots(figsize=(6,5))
plt.rcParams.update({'font.size': 12})
for fold, (train, test) in enumerate(cv.split(X_train,y_train)):
    print("Fold ",+i)
    VAE=autoencoder.predict(X_train[train])
    GAselectedfea=GA(VAE,y_train[train])
    Gfeature=GAdata(VAE,GAselectedfea)
    Gfeature=np.array(Gfeature).astype(float)

    classifier.fit(Gfeature,y_train[train])
    Test1=autoencoder.predict(X_train[test])
    Gfeaturetest=GAdata(Test1,GAselectedfea)
    Gfeaturetest=np.array(Gfeaturetest).astype(float)
    prediction = classifier.predict_proba(Gfeaturetest)[:,1]
    fpr, tpr, t = roc_curve(y_train[test], prediction)
    mean_tpr += np.interp(mean_fpr, fpr, tpr) # one dimensional interpolation
    mean_tpr[0] = 0.0
    roc_auc = roc_auc_score(y_train[test],prediction)
    plt.plot(fpr, tpr,linestyle ='solid', alpha=1,lw=1, label='ROC fold %d (AUC = %0.4f)' % (i, roc_auc))
    i= i+1
mean_tpr/= 5
mean_tpr[-1] = 1.0
mean_auc = metrics.auc(mean_fpr, mean_tpr)
plt.plot(mean_fpr, mean_tpr, color='black',linestyle ='--',
         label=r'Mean ROC (AUC = %0.4f)' % (mean_auc),alpha=1)
plt.xlabel('False Positive Rate(1-specificity)')
plt.ylabel('True Positive Rate(Sensitivity)')
plt.title('Receiver Operating Characteristic Curve:5-Fold Cross validation')
plt.legend(loc="lower right")
plt.legend()
plt.show()

cv = StratifiedKFold(n_splits=10,shuffle=True)
classifier = RandomForestClassifier(n_estimators=100,random_state=48)

mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)
i=1
fig, ax = plt.subplots(figsize=(6,5))
plt.rcParams.update({'font.size': 12})
for fold, (train, test) in enumerate(cv.split(X_train,y_train)):
    print("Fold ",+i)
    VAE=autoencoder.predict(X_train[train])
    GAselectedfea=GA(VAE,y_train[train])
    Gfeature=GAdata(VAE,GAselectedfea)
    Gfeature=np.array(Gfeature).astype(float)

    classifier.fit(Gfeature,y_train[train])
    Test1=autoencoder.predict(X_train[test])
    Gfeaturetest=GAdata(Test1,GAselectedfea)
    Gfeaturetest=np.array(Gfeaturetest).astype(float)
    prediction = classifier.predict_proba(Gfeaturetest)[:,1]
    fpr, tpr, t = roc_curve(y_train[test], prediction)
    mean_tpr += np.interp(mean_fpr, fpr, tpr) # one dimensional interpolation
    mean_tpr[0] = 0.0
    roc_auc = roc_auc_score(y_train[test],prediction)
    plt.plot(fpr, tpr,linestyle ='solid', alpha=1,lw=1, label='ROC fold %d (AUC = %0.4f)' % (i, roc_auc))
    i= i+1
mean_tpr/= 10
mean_tpr[-1] = 1.0
mean_auc = metrics.auc(mean_fpr, mean_tpr)
plt.plot(mean_fpr, mean_tpr, color='black',linestyle ='--',
         label=r'Mean ROC (AUC = %0.4f)' % (mean_auc),alpha=1)
plt.xlabel('False Positive Rate(1-specificity)')
plt.ylabel('True Positive Rate(Sensitivity)')
plt.title('Receiver Operating Characteristic Curve:10-Fold Cross validation')
plt.legend(loc="lower right")
plt.legend()
plt.show()